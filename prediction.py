#Paritosh Jha
# -*- coding: utf-8 -*-
"""HW4_New.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1x7iFIlRyOipyTFUiNbLITlK3ZnxN5WG7
"""

#Define paths 
training_dir = './hw4_train'
testing_dir = './hw4_test'
model_path = './models/saved_model.hdf5'

#dont pretrain model 
load_pretrained = False

import tensorflow as tf
from tensorflow.keras import Model
from tensorflow.keras.models import Sequential
from tensorflow.keras import layers
from tensorflow.keras.utils import image_dataset_from_directory as import_dataset
from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau
from tensorflow.data import Dataset, AUTOTUNE
from numpy import argmax, concatenate
import matplotlib.pyplot as graphplot

class TrendClassifier(object):
    val_data: Dataset
    test_data: Dataset
    epochs: int
    batch_size: int
    model: Model
    model_path: str

    def __init__(self, model, params, model_path):
        
        self.model = model
        self.model_path = model_path
        self.batch_size = params['batch_size']
        self.epochs = params['training_epochs']
        self.checkpoint_save = ModelCheckpoint(
            self.model_path,
            save_best_only=True,
            monitor='val_loss',
            mode='min')
        self.reduce_lr_loss = ReduceLROnPlateau(
            monitor='val_loss',
            factor=0.1,
            patience=15,
            verbose=1,
            mode='min')

    def train(self, train_data_dir):
        """
        Main training routine
            - Load the data from the specified directory
            - Split into training and validation sets
            - Train the model
            - Plot the training metrics
        """
        train_data = import_dataset(
            train_data_dir,
            shuffle=True,
            image_size=(28, 28),
            color_mode='grayscale',
            batch_size=self.batch_size,
            labels='inferred',
            label_mode='int',
            subset='training',
            validation_split=0.2,
            seed=42).cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)
        val_data = import_dataset(
            train_data_dir,
            shuffle=False,
            image_size=(28, 28),
            color_mode='grayscale',
            batch_size=self.batch_size,
            labels='inferred',
            label_mode='int',
            subset='validation',
            validation_split=0.2,
            seed=42).cache().prefetch(buffer_size=AUTOTUNE)
        self.model.compile(
            optimizer='adam',
            loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
            metrics=['accuracy'])
        fit_data = self.model.fit(
            train_data,
            validation_data=val_data,
            epochs=self.epochs,
            callbacks=[self.checkpoint_save,
                       self.reduce_lr_loss])
        self.plot_training_metrics(
            fit_data=fit_data,
            metrics={'accuracy', 'loss'})

    def save(self):
        self.model.save(self.model_path)
    
    def load(self):  

        self.model = tf.saved_model.load(self.model_path)

    #plot the rates 
    @staticmethod
    def plot_training_metrics(fit_data, metrics):

        graphplot.figure(figsize=(10, 10))
        for idx, metric in enumerate(metrics):
            graphplot.subplot(1, len(metrics), idx + 1)
            graphplot.title(metric.capitalize())
            graphplot.plot(fit_data.history[metric], label='Training ' + metric)
            graphplot.plot(fit_data.history['val_' + metric], label='Validation ' + metric)
            graphplot.legend()
        graphplot.show()

    def predict(self, test_data_dir):
        
        """
        test_data = import_dataset(test_data_dir,
                                   shuffle=False,
                                   batch_size=None,
                                   image_size=(28, 28),
                                   color_mode='grayscale',
                                   labels=None,)
        """
        
        imgs = []
        for i in range(10000):
          file_name = f"./hw4_test/{i}.png"
          img = tf.io.read_file(file_name)
          img = tf.image.decode_png(img,channels=1)
          img = img.numpy()
          imgs.append(img)
        
        

        y = concatenate([tf.expand_dims(image, 0) for image in imgs], axis=0)
        print('test data shape', y.shape)
        predictions = self.model.predict(y)
        as_str = []
        for prediction in predictions:
            as_str.append(str(argmax(prediction)))
        return as_str

# model parameters
params = {
    'batch_size': 32,       
    'training_epochs': 65 
}

# 2D convolutional layer with max pooling and dropout at 10%

# model definition
model = Sequential([
    layers.Rescaling(1. / 255, input_shape=(28, 28, 1)),
    layers.RandomRotation(0.05),
    layers.Flatten(),
    layers.Dense(100, activation='relu'),
    layers.Dense(10)
])

# load model
myTrendClassifier = TrendClassifier(model, params, model_path)


if load_pretrained:
    myTrendClassifier.load()
else:
    myTrendClassifier.train(training_dir)
    myTrendClassifier.save()

import numpy

mypredictions = myTrendClassifier.predict(testing_dir)
print('Generating ./prediction.txt with', len(mypredictions), 'predictions')
with open('./prediction.txt', 'w') as f:
    for count, mypred in enumerate(mypredictions):
        f.write(mypred + '\n' if count != (len(mypredictions) - 1) else mypred)
# for Home Work submission
#!zip -r './prediction.zip' './prediction.txt'
print('Program finished')